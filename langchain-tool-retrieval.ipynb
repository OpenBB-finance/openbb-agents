{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenBB Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies, in specific langchain\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import hub\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import RetryWithErrorOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"  # Avoid some warnings from HuggingFace\n",
    "\n",
    "# Set up OpenAI API key\n",
    "import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = \"\"\n",
    "\n",
    "# Set up OpenBB Personal Access Token from https://my.openbb.co/app/platform/pat\n",
    "from openbb import obb\n",
    "from utils import map_openbb_collection_to_langchain_tools  # provides access to OpenBB Tools\n",
    "obb.account.login(pat=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OpenBB tools for retrieval\n",
    "\n",
    "The following will return all OpenBB tools that we want our agent to have access. This matches the layout architecture defined by OpenBB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 27 OpenBB tools has been prepared for function calling\n",
      "\n",
      "Processing each OpenBB tool description into a list of docs...\n",
      "\n",
      "Create embeddings for each of these OpenBB tool descriptions...\n"
     ]
    }
   ],
   "source": [
    "# can also give a single string, but since we want our agent to have more context we provide access to more functions\n",
    "# TODO: In future, we might implement a  `universal_openbb_tools` collection that allows quick LLM access to all OpenBB functions. \n",
    "# We don't run into context size issues doing this, since we rely on embeddings for our tools., the context size is not an issue. \n",
    "# However, with many tools in a vector store, retrieval may become a challenge.\n",
    "openbb_tools = map_openbb_collection_to_langchain_tools(\n",
    "    openbb_commands_root = [\n",
    "        \"/equity/fundamental\",\n",
    "        \"/equity/compare\",\n",
    "        \"/equity/estimates\"\n",
    "    ]\n",
    ")\n",
    "print(f\"A total of {len(openbb_tools)} OpenBB tools has been prepared for function calling\\n\")\n",
    "\n",
    "\n",
    "print(\"Processing each OpenBB tool description into a list of docs...\\n\")\n",
    "# Parse the description (i.e. docstring + output fields) for each of these tools\n",
    "docs = [\n",
    "    Document(page_content=t.description, metadata={\"index\": i})\n",
    "    for i, t in enumerate(openbb_tools)\n",
    "]\n",
    "\n",
    "print(\"Create embeddings for each of these OpenBB tool descriptions...\")\n",
    "# Create embeddings from each of these function descriptions\n",
    "# this will be important for when we want the agent to know what\n",
    "# function to use for a particular query\n",
    "vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tools(query):\n",
    "    \"Retrieve the most relevant documents to a query.\"\n",
    "    # Set a docs retriever that looks for a score_threshold of 0.65, this means that if the search retrieval\n",
    "    # is confident in a few endpoints we don't rule out any due to a hardcoded number of docs to be retrieved\n",
    "    # Empirically this is what we found that worked best for our particular application. This may vary depending on the use case.\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={'score_threshold': 0.65}\n",
    "    )\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # This is a fallback mechanism in case the threshold is too high, causing too few tools to be returned.\n",
    "    # In this case, we fall back to getting the top k=2 results with higher similarity scores.\n",
    "    if len(docs) < 2:\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": 2}\n",
    "        )\n",
    "        \n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "    return [openbb_tools[d.metadata[\"index\"]] for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our new vector store that contains our tool embeddings to build some intuition around how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools retrieved for 'market cap':\n",
      "  /equity/fundamental/metrics - Key Metrics. Key metrics for a given company.\n",
      "  /equity/fundamental/multiples - Equity Valuation Multiples. Valuation multiples for a stock ticker.\n",
      "  /equity/fundamental/overview - Company Overview. General information about a company.\n",
      "  /equity/fundamental/income - Income Statement. Report on a company's financial performance.\n",
      "\n",
      "Tools retrieved for 'peers':\n",
      "  /equity/compare/peers - Equity Peers. Company peers.\n",
      "  /equity/fundamental/metrics - Key Metrics. Key metrics for a given company.\n"
     ]
    }
   ],
   "source": [
    "print(\"Tools retrieved for 'market cap':\")\n",
    "fetched_tools = get_tools(\"market cap\")\n",
    "for tool in fetched_tools:\n",
    "    print(\"  \" + tool.name + \" - \" + tool.description.split('\\n')[0])\n",
    "\n",
    "print(\"\\nTools retrieved for 'peers':\")\n",
    "fetched_tools = get_tools(\"peers\")\n",
    "for tool in fetched_tools:\n",
    "    print(\"  \" + tool.name + \" - \" + tool.description.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-powered Financial Analyst\n",
    "\n",
    "There are two example prompts to choose from below (but feel free to try your own), one requires linear reasoning (where future answers may depend on previous answers), while the other requires independent reasoning (fetching and combining different pieces of independent information).\n",
    "\n",
    "These prompts answer 2 very different scenarios and in this notebook, we are going to demonstrate that our OpenBB agent is capable of handling both efficiently, utilizing the same architecture\n",
    "\n",
    "- **Prompt 1** - This prompt is very deterministic which allows us to access right or wrong immediately because we can check the facts.\n",
    "It also involves a few complex operations such as extracting a list of tickers from an endpoint and iterating through that list using a different endpoint. Then based on those outputs, a reasoning is made.\n",
    "\n",
    "- **Prompt 2** - This prompt is not deterministic and allows us to leverage LLMs to provide alpha by uncovering insights that would be hard for a human to discover. Instead of telling the agent what to do, we expect the agent to provide a reasoning of what it would do to perform a typical analyst task, without guardrails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1\n",
    "PROMPT =  \"\"\"\\\n",
    "Check what are TSLA peers. From those, check which one has the highest market cap.\n",
    "Then, on the ticker that has the highest market cap get the most recent price target estimate from an analyst,\n",
    "and tell me who it was and on what date the estimate was made.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt 2\n",
    "# PROMPT = \"Perform a fundamentals financial analysis of AMZN using the most recently available data. What do you find that's interesting?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task decomposition\n",
    "\n",
    "The goal is to simplify the user prompt into simpler tasks and execute those for more accuracy.\n",
    "\n",
    "1. Break a larger query down into subquery.\n",
    "2. Then for each subquery create a set of keywords that allow you to fetch the right tool to execute that same subquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We experiment with Pydantic here to enforce structured output\n",
    "# \"Pydantic is all you need\" - https://www.youtube.com/watch?v=yj-wSRJwrrc\n",
    "\n",
    "class SubQuestion(BaseModel):\n",
    "    \"Pydantic data model we want each subquestion to have, including each field and what they represent\"\n",
    "    id: int = Field(\n",
    "        description=\"The unique ID of the subquestion.\"\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"The subquestion itself.\"\n",
    "    )\n",
    "    query: str = Field(\n",
    "        description=\"The query to pass to the `fetch_tools` function to retrieve the appropriate tool to answer the question.\"\n",
    "    )\n",
    "    depends_on: list[int] = Field(\n",
    "        description=\"The list of subquestion ids whose answer is required to answer this subquestion.\",\n",
    "        default=[]\n",
    "    )\n",
    "\n",
    "class SubQuestionList(BaseModel):\n",
    "    \"Pydantic data model output we want to enforce, which is a list of the previous SubQuestion Pydantic model\"\n",
    "    subquestions: list[SubQuestion] = Field(\n",
    "        description=\"The list of SubQuestion objects.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_decomposition(task: str):\n",
    "    \"Break a larger query down into subquery. Then for each subquery create a set of keywords that allow you to fetch the right tool to execute that same subquery.\"\n",
    "    subquestion_parser = PydanticOutputParser(pydantic_object=SubQuestionList)\n",
    "    \n",
    "    system_message = \"\"\"\\\n",
    "    You are a world-class state-of-the-art agent.\n",
    "    \n",
    "    You can access multiple tools, via a \"fetch_tools\" function that will retrieve the necessary tools.\n",
    "    The `fetch_tools` function accepts a string of keywords as input specifying the type of tool to retrieve.\n",
    "    Each retrieved tool represents a different data source or API that can retrieve the required data.\n",
    "    \n",
    "    Your purpose is to help answer a complex user question by generating a list of subquestions,\n",
    "    as well as the corresponding keyword query to the \"fetch_tools\" function\n",
    "    to retrieve the relevant tools to answer each corresponding subquestion.\n",
    "    You must also specify the dependencies between subquestions, since sometimes one\n",
    "    subquestion will require the outcome of another in order to fully answer.\n",
    "    \n",
    "    These are the guidelines you consider when completing your task:\n",
    "    * Be as specific as possible\n",
    "    * Avoid using acronyms\n",
    "    * The subquestions should be relevant to the user's question\n",
    "    * The subquestions should be answerable by the tools retrieved by the query to `fetch_tools`\n",
    "    * You can generate multiple subquestions\n",
    "    * You don't need to query for a tool if you don't think it's relevant\n",
    "    * A subquestion may not depend on a subquestion that proceeds it (i.e. comes after it.)\n",
    "    \n",
    "    ## Output format\n",
    "    {format_instructions}\n",
    "    \n",
    "    ### Example responses\n",
    "    ```json\n",
    "    {{\"subquestions\": [\n",
    "        {{\n",
    "            \"id\": 1,\n",
    "            \"question\": \"What are the latest financial statements of AMZN?\", \n",
    "            \"query\": \"financial statements\",\n",
    "            \"depends_on\": []\n",
    "        }}, \n",
    "        {{\n",
    "            \"id\": 2,\n",
    "            \"question\": \"What is the most recent revenue and profit margin of AMZN?\", \n",
    "            \"query\": \"revenue profit margin ratios\",\n",
    "            \"depends_on\": []\n",
    "        }}, \n",
    "        {{\n",
    "            \"id\": 3,\n",
    "            \"question\": \"What is the current price to earnings (P/E) ratio of AMZN?\", \n",
    "            \"query\": \"ratio price to earnings\",\n",
    "            \"depends_on\": []\n",
    "        }}, \n",
    "        {{\n",
    "            \"id\": 4,\n",
    "            \"question\": \"Who are the peers of AMZN?\", \n",
    "            \"query\": \"peers\",\n",
    "            \"depends_on\": []\n",
    "        }},\n",
    "        {{\n",
    "            \"id\": 5,\n",
    "            \"question\": \"Which of AMZN's peers have the largest market cap?\", \n",
    "            \"query\": \"market cap\",\n",
    "            \"depends_on\": [4]\n",
    "        }}\n",
    "    ]}}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    human_message = \"\"\"\\\n",
    "        ## User Question\n",
    "        {input}\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_message),\n",
    "            (\"human\", human_message),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(\n",
    "        format_instructions=subquestion_parser.get_format_instructions()\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4\"\n",
    "    )  # gpt-3.5-turbo works well, but gpt-4-1106-preview isn't good at returning JSON.\n",
    "    \n",
    "    subquestion_chain = {\"input\": lambda x: x[\"input\"]} | prompt | llm | subquestion_parser\n",
    "\n",
    "    subquestion_list = subquestion_chain.invoke({\"input\": task})\n",
    "\n",
    "    return subquestion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Who are the peers of Tesla (TSLA)?\n",
      "  Query: Tesla peers\n",
      "2 - Which of TSLA's peers has the largest market cap?\n",
      "  Query: market cap\n",
      "  Depends on: [1]\n",
      "3 - What is the most recent price target estimate for the company with the largest market cap?\n",
      "  Query: price target estimate\n",
      "  Depends on: [2]\n",
      "4 - Who made the most recent price target estimate for the company with the largest market cap?\n",
      "  Query: analyst name price target estimate\n",
      "  Depends on: [2]\n",
      "5 - On what date was the most recent price target estimate for the company with the largest market cap made?\n",
      "  Query: date price target estimate\n",
      "  Depends on: [2]\n"
     ]
    }
   ],
   "source": [
    "subquestion_list = task_decomposition(PROMPT)\n",
    "\n",
    "# Shows the result from task decomposition\n",
    "for subquestion in subquestion_list.subquestions:\n",
    "    print(f\"{subquestion.id} - {subquestion.question}\")\n",
    "    print(f\"  Query: {subquestion.query}\")\n",
    "    if subquestion.depends_on:\n",
    "        print(f\"  Depends on: {subquestion.depends_on}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool retrieval\n",
    "\n",
    "Use the previously generated queries in order to fetch the tools necessary to answer the subquestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Who are the peers of Tesla (TSLA)?\n",
      "  Query: Tesla peers\n",
      "  Depends on: []\n",
      "  Fetched tools:\n",
      "    /equity/compare/peers: Equity Peers. Company peers.\n",
      "    /equity/fundamental/metrics: Key Metrics. Key metrics for a given company.\n",
      "\n",
      "2 - Which of TSLA's peers has the largest market cap?\n",
      "  Query: market cap\n",
      "  Depends on: [1]\n",
      "  Fetched tools:\n",
      "    /equity/fundamental/metrics: Key Metrics. Key metrics for a given company.\n",
      "    /equity/fundamental/multiples: Equity Valuation Multiples. Valuation multiples for a stock ticker.\n",
      "    /equity/fundamental/overview: Company Overview. General information about a company.\n",
      "    /equity/fundamental/income: Income Statement. Report on a company's financial performance.\n",
      "\n",
      "3 - What is the most recent price target estimate for the company with the largest market cap?\n",
      "  Query: price target estimate\n",
      "  Depends on: [2]\n",
      "  Fetched tools:\n",
      "    /equity/estimates/consensus: Price Target Consensus. Price target consensus data.\n",
      "    /equity/estimates/price_target: Price Target. Price target data.\n",
      "    /equity/estimates/historical: Historical Analyst Estimates. Analyst stock recommendations.\n",
      "    /equity/fundamental/multiples: Equity Valuation Multiples. Valuation multiples for a stock ticker.\n",
      "\n",
      "4 - Who made the most recent price target estimate for the company with the largest market cap?\n",
      "  Query: analyst name price target estimate\n",
      "  Depends on: [2]\n",
      "  Fetched tools:\n",
      "    /equity/estimates/historical: Historical Analyst Estimates. Analyst stock recommendations.\n",
      "    /equity/estimates/consensus: Price Target Consensus. Price target consensus data.\n",
      "    /equity/estimates/price_target: Price Target. Price target data.\n",
      "    /equity/fundamental/historical_eps: Historical earnings-per-share for a given company.\n",
      "\n",
      "5 - On what date was the most recent price target estimate for the company with the largest market cap made?\n",
      "  Query: date price target estimate\n",
      "  Depends on: [2]\n",
      "  Fetched tools:\n",
      "    /equity/estimates/consensus: Price Target Consensus. Price target consensus data.\n",
      "    /equity/estimates/price_target: Price Target. Price target data.\n",
      "    /equity/estimates/historical: Historical Analyst Estimates. Analyst stock recommendations.\n",
      "    /equity/fundamental/historical_eps: Historical earnings-per-share for a given company.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subquestions_and_tools = []\n",
    "for subquestion in subquestion_list.subquestions:\n",
    "    tools = get_tools(subquestion.query)\n",
    "    subquestions_and_tools.append(\n",
    "        {   \"id\": subquestion.id,\n",
    "            \"subquestion\": subquestion.question,\n",
    "            \"query\": subquestion.query,\n",
    "            \"tools\": tools,\n",
    "            \"depends_on\": subquestion.depends_on,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Shows the result from the fetched tools for each subquestion's query \n",
    "for subq in subquestions_and_tools:\n",
    "    print(f\"{subq['id']} - {subq['subquestion']}\")\n",
    "    print(f\"  Query: {subq['query']}\")\n",
    "    if subquestion.depends_on:\n",
    "        print(f\"  Depends on: {subq['depends_on']}\")\n",
    "    print(\"  Fetched tools:\")\n",
    "    for tool in subq[\"tools\"]:\n",
    "        print(\"    \" + tool.name + \": \" + tool.description.split('\\n')[0])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent to execute on each subquestion\n",
    "\n",
    "The ReAct agent answers each of the subquestions. This is done by providing it with the subquestion and its corresponding fetched tools.\n",
    "\n",
    "ReAct paper: https://react-lm.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_react_agent(tools):\n",
    "    \"Define a ReAct agent bound with specific tools.\"\n",
    "    # This retrieves the ReAct agent chat prompt template available in Langchain Hub\n",
    "    # https://smith.langchain.com/hub/hwchase17/react-json?organizationId=10beea65-e722-5aa1-9f93-034c22e3cd6e\n",
    "    prompt = hub.pull(\"hwchase17/react-multi-input-json\")\n",
    "    # Replace the 'tools' and 'tool_names' content of the prompt with information given to the agent\n",
    "    # Note that tool_names is a field available in each tool, so it can be inferred from same argument\n",
    "    prompt = prompt.partial(\n",
    "        tools=render_text_description_and_args(tools),\n",
    "        tool_names=\", \".join([t.name for t in tools]),\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4-1106-preview\").bind(stop=[\"\\nObservation\"])\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | JSONAgentOutputParser()\n",
    "    )\n",
    "\n",
    "    # Agent executor with access to the chain and tools at its disposal\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        tools=tools,\n",
    "        verbose=False,  # <-- set this to False to cut down on output spam. But it's useful for debugging!\n",
    "        return_intermediate_steps=False,\n",
    "        handle_parsing_errors=True,\n",
    "    )\n",
    "    return agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are the peers of Tesla (TSLA)?\n",
      "- The peers of Tesla (TSLA) are XPeng Inc. (XPEV), Li Auto Inc. (LI), Rivian Automotive, Inc. (RIVN), Lucid Group, Inc. (LCID), General Motors Company (GM), NIO Inc. (NIO), Ford Motor Company (F), Fisker Inc. (FSR), and Mullen Automotive, Inc. (MULN).\n",
      "- These companies are considered peers because they operate in the same sector and industry, focusing on electric vehicles and automotive manufacturing, which is the primary business of Tesla.\n",
      "\n",
      "\n",
      "Which of TSLA's peers has the largest market cap?\n",
      "- General Motors Company (GM) has the largest market cap among Tesla's peers, with a market cap of $43,275,565,978.\n",
      "- The market caps of other peers are as follows: Li Auto Inc. (LI) at $37,031,658,909, Rivian Automotive, Inc. (RIVN) at $15,920,206,546, NIO Inc. (NIO) at $12,930,495,049, Ford Motor Company (F) at $40,343,346,985, XPeng Inc. (XPEV) at $11,916,511,602, Lucid Group, Inc. (LCID) at $9,661,394,668, Fisker Inc. (FSR) at $553,878,493, and Mullen Automotive, Inc. (MULN) at $62,790,894.\n",
      "- The data was retrieved from the company overview tool, which provides the latest available market capitalization figures for each company.\n",
      "\n",
      "\n",
      "What is the most recent price target estimate for the company with the largest market cap?\n",
      "- The most recent price target estimate for General Motors Company (GM) is $37.0.\n",
      "- This estimate was provided by analyst Dan Levy from Barclays.\n",
      "- The estimate was made on November 1, 2023.\n",
      "\n",
      "\n",
      "Who made the most recent price target estimate for the company with the largest market cap?\n",
      "- The most recent price target estimate for General Motors Company (GM) was made by Dan Levy from Barclays on November 1, 2023.\n",
      "- This observation is based on the most recent data retrieved from the price target tool.\n",
      "\n",
      "\n",
      "On what date was the most recent price target estimate for the company with the largest market cap made?\n",
      "- The most recent price target estimate for General Motors Company (GM) was made on November 1, 2023.\n",
      "- This estimate was provided by analyst Dan Levy from Barclays with a price target of $37.0.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Go through each subquestion and create an agent with the necessary tools and context to execute on it\\n\n",
    "for i, subquestion in enumerate(subquestions_and_tools):\n",
    "\n",
    "    # We handle each dependency manually since we don't want agents to share memory as this can go over context length\n",
    "    deps = [dep for dep in subquestions_and_tools if dep[\"id\"] in subquestion[\"depends_on\"]]\n",
    "\n",
    "    dependencies = \"\"\n",
    "    for dep in deps:\n",
    "        dependencies += \"subquestion: \" + dep[\"subquestion\"] + \"\\n\"\n",
    "        # if for some reason there's no temporal dependency between the agents being run\n",
    "        # this ensures the code doesn't break here\n",
    "        if \"observation\" in dep:\n",
    "            dependencies += \"observations:\\n\" + str(dep[\"observation\"]) + \"\\n\\n\"\n",
    "\n",
    "    input = f\"\"\"\\\n",
    "Given the following high-level question: {PROMPT}\n",
    "Answer the following subquestion: {subquestion['subquestion']}\n",
    "\n",
    "Give your answer in a bullet-point list.\n",
    "Explain your reasoning, and make reference to and provide the relevant retrieved data as part of your answer.\n",
    "\n",
    "Remember to use the tools provided to you to answer the question, and STICK TO THE INPUT SCHEMA.\n",
    "\n",
    "Example output format:\n",
    "```\n",
    "- <the first observation, insight, and/or conclusion> \n",
    "- <the second observation, insight, and/or conclusion> \n",
    "- <the third observation, insight, and/or conclusion> \n",
    "... REPEAT AS MANY TIMES AS NECESSARY TO ANSWER THE SUBQUESTION.\n",
    "```\n",
    "\n",
    "If necessary, make use of the following subquestions and their answers to answer your subquestion:\n",
    "{dependencies}\n",
    "\n",
    "Return only your answer as a bulleted list as a single string. Don't respond with JSON or any other kind of data structure.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = langchain_react_agent(tools=subquestion[\"tools\"]).invoke({\"input\": input})\n",
    "        output = result[\"output\"]\n",
    "    except Exception as err:  # Terrible practice, but it'll do for now.\n",
    "        print(err)\n",
    "        # We'll include the error message in the future\n",
    "        output = \"I was unable to answer the subquestion using the available tool.\" \n",
    "\n",
    "\n",
    "    # This is very cheeky but we are basically going into the subquestions_and_tools and for this current subquestion\n",
    "    # we are adding the output as an observation. This is important because then above we do the dependencies check-up\n",
    "    # which allows us to retrieve the correct output to be used in another subquestion.\n",
    "    # Note: this works because subquestions are done in order to execute prompt. Otherwise it wouldn't since we would\n",
    "    # be looking for an \"observation\" that doesn't exist yet.\n",
    "    subquestion[\"observation\"] = output\n",
    "\n",
    "    print(subquestion['subquestion'])\n",
    "    if isinstance(output, dict):\n",
    "        for val in output.values():\n",
    "            print(val)\n",
    "    else:\n",
    "        print(output)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verdict\n",
    "\n",
    "To combine all of the subquestion answers to generate a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_subquestions_and_answers(subquestions):\n",
    "    \"Combines all subquestions and their answers\"\n",
    "    output = \"\"\n",
    "    for subquestion in subquestions:\n",
    "        output += \"Subquestion: \" + subquestion[\"subquestion\"] + \"\\n\"\n",
    "        output += \"Observations: \\n\" + str(subquestion[\"observation\"]) + \"\\n\\n\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verdict(question: str, subquestions: dict):\n",
    "    \"Based on the high-level question, it combines the subquestions and their answers to give one final concise answer\"\n",
    "    system_message = \"\"\"\\\n",
    "        Given the following high-level question: \n",
    "    \n",
    "        {input}\n",
    "    \n",
    "        And the following subquestions and subsequent observations:\n",
    "    \n",
    "        {subquestions}\n",
    "    \n",
    "        Answer the high-level question. Give your answer in a bulleted list.\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system_message)])\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4\")  # Let's use the big model for the final answer.\n",
    "    \n",
    "    final_chain = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"subquestions\": lambda x: render_subquestions_and_answers(x[\"subquestions\"]),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "    )\n",
    "    \n",
    "    result = final_chain.invoke({\"input\": question, \"subquestions\": subquestions})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The peers of Tesla (TSLA) are XPeng Inc. (XPEV), Li Auto Inc. (LI), Rivian Automotive, Inc. (RIVN), Lucid Group, Inc. (LCID), General Motors Company (GM), NIO Inc. (NIO), Ford Motor Company (F), Fisker Inc. (FSR), and Mullen Automotive, Inc. (MULN).\n",
      "- Among these peers, General Motors Company (GM) has the largest market capitalization at $43,275,565,978.\n",
      "- The most recent price target estimate for General Motors Company (GM) is $37.0.\n",
      "- This estimate was provided by analyst Dan Levy from Barclays.\n",
      "- The price target estimate was made on November 1, 2023.\n"
     ]
    }
   ],
   "source": [
    "result = verdict(\n",
    "    question=PROMPT,\n",
    "    subquestions=subquestions_and_tools\n",
    ")\n",
    "print(result.content)  # Et voila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>symbol</td>\n",
       "      <td>GM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>price</td>\n",
       "      <td>31.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beta</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vol_avg</td>\n",
       "      <td>16439042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mkt_cap</td>\n",
       "      <td>43275565978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0            1\n",
       "0   symbol           GM\n",
       "1    price         31.6\n",
       "2     beta         1.49\n",
       "3  vol_avg     16439042\n",
       "4  mkt_cap  43275565978"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obb.equity.fundamental.overview(\"GM\").to_df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>published_date</th>\n",
       "      <th>news_url</th>\n",
       "      <th>news_title</th>\n",
       "      <th>analyst_name</th>\n",
       "      <th>analyst_company</th>\n",
       "      <th>price_target</th>\n",
       "      <th>adj_price_target</th>\n",
       "      <th>price_when_posted</th>\n",
       "      <th>news_publisher</th>\n",
       "      <th>news_base_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GM</td>\n",
       "      <td>2023-11-01 02:32:00</td>\n",
       "      <td>https://www.streetinsider.com/Upgrades/Barclay...</td>\n",
       "      <td>Barclays Upgrades General Motors (GM) to Overw...</td>\n",
       "      <td>Dan Levy</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.2</td>\n",
       "      <td>StreetInsider</td>\n",
       "      <td>streetinsider.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol      published_date  \\\n",
       "0     GM 2023-11-01 02:32:00   \n",
       "\n",
       "                                            news_url  \\\n",
       "0  https://www.streetinsider.com/Upgrades/Barclay...   \n",
       "\n",
       "                                          news_title analyst_name  \\\n",
       "0  Barclays Upgrades General Motors (GM) to Overw...     Dan Levy   \n",
       "\n",
       "  analyst_company  price_target  adj_price_target  price_when_posted  \\\n",
       "0        Barclays          37.0              37.0               28.2   \n",
       "\n",
       "  news_publisher      news_base_url  \n",
       "0  StreetInsider  streetinsider.com  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obb.equity.estimates.price_target(\"GM\").to_df().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
