{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extensions to add: etf@1.0.0rc0, oecd@1.0.0rc0, economy@1.0.0rc0, fmp@1.0.0rc0, crypto@1.0.0rc0, benzinga@1.0.0rc0, equity@1.0.0rc0, finra@1.0.0rc0, derivatives@1.0.0rc0, wsj@1.0.0rc0, intrinio@1.0.0rc0, regulators@1.0.0rc0, sec@1.0.0rc0, fixedincome@1.0.0rc0, index@1.0.0rc0, news@1.0.0rc0, currency@1.0.0rc0, government_us@1.0.0rc0, polygon@1.0.0rc0, tradingeconomics@1.0.0rc0, tiingo@1.0.0rc0, fred@1.0.0rc0\n",
      "Extensions to remove: fmp@1.0.0b2, oecd@1.0.0b2, intrinio@1.0.0b2, fixedincome@1.0.0b2, currency@1.0.0b2, tradingeconomics@1.0.0b2, etf@1.0.0b2, crypto@1.0.0b2, fred@1.0.0b2, benzinga@1.0.0b2, derivatives@1.0.0b2, polygon@1.0.0b2, government_us@1.0.0b2, wsj@1.0.0b2, index@1.0.0b2, regulators@1.0.0b2, news@1.0.0b2, sec@1.0.0b2, equity@1.0.0b2, finra@1.0.0b2, economy@1.0.0b2, tiingo@1.0.0b2\n",
      "\n",
      "Building...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UserSettings\n",
       "\n",
       "id: 0656c40b-3d15-71b4-8000-970ea6b45ad7\n",
       "profile: {'hub_session': {'username': 'didier', 'email': 'didier.lopes@openbb.finance', 'primary_usage': 'personal', 'user_uuid': 'a7ac8796-9328-4fd5-bd12-2c528855655b', 'token_type': 'bearer', 'access_token': SecretStr('**********')}}\n",
       "credentials: {'benzinga_api_key': None, 'intrinio_api_key': None, 'fred_api_key': SecretStr('**********'), 'tradingeconomics_api_key': None, 'fmp_api_key': SecretStr('**********'), 'polygon_api_key': SecretStr('**********'), 'tiingo_token': None}\n",
       "preferences: {'data_directory': '/Users/didierlopes/OpenBBUserData', 'export_directory': '/Users/didierlopes/OpenBBUserData/exports', 'user_styles_directory': '/Users/didierlopes/OpenBBUserData/styles/user', 'cache_directory': '/Users/didierlopes/OpenBBUserData/cache', 'charting_extension': 'openbb_charting', 'chart_style': 'dark', 'plot_enable_pywry': True, 'plot_pywry_width': 1400, 'plot_pywry_height': 762, 'plot_open_export': False, 'table_style': 'dark', 'request_timeout': 15, 'metadata': True, 'output_type': 'OBBject'}\n",
       "defaults: {'routes': {}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dependencies, in specific langchain\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import hub\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import RetryWithErrorOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"  # Avoid some warnings from HuggingFace\n",
    "\n",
    "# Set up OpenAI API key\n",
    "import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = \"\"\n",
    "\n",
    "# Set up OpenBB Personal Access Token from https://my.openbb.co/app/platform/pat\n",
    "from openbb import obb\n",
    "from utils import map_openbb_collection_to_langchain_tools  # provides access to OpenBB Tools\n",
    "obb.account.login(pat=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openbb_tools = map_openbb_collection_to_langchain_tools(\n",
    "    openbb_commands_root = [\n",
    "        \"/equity/fundamental\",\n",
    "        \"/equity/compare\",\n",
    "        \"/equity/estimates\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query =  \"\"\"\\\n",
    "Check what are TSLA peers. From those, check which one has the highest market cap.\n",
    "Then, on the ticker that has the highest market cap get the most recent price target estimate from an analyst,\n",
    "and tell me who it was and on what date the estimate was made.\n",
    "\"\"\"\n",
    "\n",
    "# user_query = \"Perform a fundamentals financial analysis of AMZN using the most recently available data. What do you find that's interesting?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- The peers of Tesla Inc (TSLA) in the automotive and electric vehicle industry include XPeng Inc. (XPEV), Li Auto Inc. (LI), Rivian Automotive, Inc. (RIVN), Lucid Group, Inc. (LCID), General Motors Company (GM), NIO Inc. (NIO), Ford Motor Company (F), Fisker Inc. (FSR), and Mullen Automotive, Inc. (MULN).\\n- Among these, General Motors Company (GM) has the highest market cap at $44,316,370,729. However, it is important to note that GM is not exclusively an electric vehicle manufacturer like the others.\\n- The most recent price target estimate for General Motors Company (GM) is $38.0.\\n- This estimate was provided by Vijay Rakesh from Mizuho Securities.\\n- The estimate was made on October 25, 2023.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "class SubQuestion(BaseModel):\n",
    "    \"Pydantic data model we want each subquestion to have, including each field and what they represent\"\n",
    "    id: int = Field(\n",
    "        description=\"The unique ID of the subquestion.\"\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"The subquestion itself.\"\n",
    "    )\n",
    "    query: str = Field(\n",
    "        description=\"The query to pass to the `fetch_tools` function to retrieve the appropriate tool to answer the question.\"\n",
    "    )\n",
    "    depends_on: list[int] = Field(\n",
    "        description=\"The list of subquestion ids whose answer is required to answer this subquestion.\",\n",
    "        default=[]\n",
    "    )\n",
    "\n",
    "class SubQuestionList(BaseModel):\n",
    "    \"Pydantic data model output we want to enforce, which is a list of the previous SubQuestion Pydantic model\"\n",
    "    subquestions: list[SubQuestion] = Field(\n",
    "        description=\"The list of SubQuestion objects.\"\n",
    "    )\n",
    "\n",
    "def task_decomposition(task: str):\n",
    "    \"Break a larger query down into subquery. Then for each subquery create a set of keywords that allow you to fetch the right tool to execute that same subquery.\"\n",
    "    subquestion_parser = PydanticOutputParser(pydantic_object=SubQuestionList)\n",
    "    \n",
    "    system_message = \"\"\"\\\n",
    "    You are a world-class state-of-the-art agent.\n",
    "    \n",
    "    You can access multiple tools, via a \"fetch_tools\" function that will retrieve the necessary tools.\n",
    "    The `fetch_tools` function accepts a string of keywords as input specifying the type of tool to retrieve.\n",
    "    Each retrieved tool represents a different data source or API that can retrieve the required data.\n",
    "    \n",
    "    Your purpose is to help answer a complex user question by generating a list of subquestions,\n",
    "    as well as the corresponding keyword query to the \"fetch_tools\" function\n",
    "    to retrieve the relevant tools to answer each corresponding subquestion.\n",
    "    You must also specify the dependencies between subquestions, since sometimes one\n",
    "    subquestion will require the outcome of another in order to fully answer.\n",
    "    \n",
    "    These are the guidelines you consider when completing your task:\n",
    "    * Be as specific as possible\n",
    "    * Avoid using acronyms\n",
    "    * The subquestions should be relevant to the user's question\n",
    "    * The subquestions should be answerable by the tools retrieved by the query to `fetch_tools`\n",
    "    * You can generate multiple subquestions\n",
    "    * You don't need to query for a tool if you don't think it's relevant\n",
    "    * A subquestion may not depend on a subquestion that proceeds it (i.e. comes after it.)\n",
    "    \n",
    "    ## Output format\n",
    "    {format_instructions}\n",
    "    \n",
    "    ### Example responses\n",
    "    ```json\n",
    "    {{\"subquestions\": [\n",
    "        {{\n",
    "            \"id\": 1,\n",
    "            \"question\": \"What are the latest financial statements of AMZN?\", \n",
    "            \"query\": \"financial statements\",\n",
    "            \"depends_on\": []\n",
    "        }}, \n",
    "        {{\n",
    "            \"id\": 2,\n",
    "            \"question\": \"What is the most recent revenue and profit margin of AMZN?\", \n",
    "            \"query\": \"revenue profit margin ratios\",\n",
    "            \"depends_on\": []\n",
    "        }}, \n",
    "        {{\n",
    "            \"id\": 3,\n",
    "            \"question\": \"What is the current price to earnings (P/E) ratio of AMZN?\", \n",
    "            \"query\": \"ratio price to earnings\",\n",
    "            \"depends_on\": []\n",
    "        }}, \n",
    "        {{\n",
    "            \"id\": 4,\n",
    "            \"question\": \"Who are the peers of AMZN?\", \n",
    "            \"query\": \"peers\",\n",
    "            \"depends_on\": []\n",
    "        }},\n",
    "        {{\n",
    "            \"id\": 5,\n",
    "            \"question\": \"Which of AMZN's peers have the largest market cap?\", \n",
    "            \"query\": \"market cap\",\n",
    "            \"depends_on\": [4]\n",
    "        }}\n",
    "    ]}}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    human_message = \"\"\"\\\n",
    "        ## User Question\n",
    "        {input}\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_message),\n",
    "            (\"human\", human_message),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(\n",
    "        format_instructions=subquestion_parser.get_format_instructions()\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4\"\n",
    "    )  # gpt-3.5-turbo works well, but gpt-4-1106-preview isn't good at returning JSON.\n",
    "    \n",
    "    subquestion_chain = {\"input\": lambda x: x[\"input\"]} | prompt | llm | subquestion_parser\n",
    "\n",
    "    subquestion_list = subquestion_chain.invoke({\"input\": task})\n",
    "\n",
    "    return subquestion_list\n",
    "\n",
    "def langchain_react_agent(tools):\n",
    "    \"Define a ReAct agent bound with specific tools.\"\n",
    "    # This retrieves the ReAct agent chat prompt template available in Langchain Hub\n",
    "    # https://smith.langchain.com/hub/hwchase17/react-json?organizationId=10beea65-e722-5aa1-9f93-034c22e3cd6e\n",
    "    prompt = hub.pull(\"hwchase17/react-multi-input-json\")\n",
    "    # Replace the 'tools' and 'tool_names' content of the prompt with information given to the agent\n",
    "    # Note that tool_names is a field available in each tool, so it can be inferred from same argument\n",
    "    prompt = prompt.partial(\n",
    "        tools=render_text_description_and_args(tools),\n",
    "        tool_names=\", \".join([t.name for t in tools]),\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4-1106-preview\").bind(stop=[\"\\nObservation\"])\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | JSONAgentOutputParser()\n",
    "    )\n",
    "\n",
    "    # Agent executor with access to the chain and tools at its disposal\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        tools=tools,\n",
    "        verbose=False,  # <-- set this to False to cut down on output spam. But it's useful for debugging!\n",
    "        return_intermediate_steps=False,\n",
    "        handle_parsing_errors=True,\n",
    "    )\n",
    "    return agent_executor\n",
    "\n",
    "def render_subquestions_and_answers(subquestions):\n",
    "    \"Combines all subquestions and their answers\"\n",
    "    output = \"\"\n",
    "    for subquestion in subquestions:\n",
    "        output += \"Subquestion: \" + subquestion[\"subquestion\"] + \"\\n\"\n",
    "        output += \"Observations: \\n\" + str(subquestion[\"observation\"]) + \"\\n\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def verdict(question: str, subquestions: dict):\n",
    "    \"Based on the high-level question, it combines the subquestions and their answers to give one final concise answer\"\n",
    "    system_message = \"\"\"\\\n",
    "        Given the following high-level question: \n",
    "    \n",
    "        {input}\n",
    "    \n",
    "        And the following subquestions and subsequent observations:\n",
    "    \n",
    "        {subquestions}\n",
    "    \n",
    "        Answer the high-level question. Give your answer in a bulleted list.\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system_message)])\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4\")  # Let's use the big model for the final answer.\n",
    "    \n",
    "    final_chain = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"subquestions\": lambda x: render_subquestions_and_answers(x[\"subquestions\"]),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "    )\n",
    "    \n",
    "    result = final_chain.invoke({\"input\": question, \"subquestions\": subquestions})\n",
    "\n",
    "    return result\n",
    "\n",
    "def openbb_agent(\n",
    "        openbb_tools: langchain.tools.base.StructuredTool,\n",
    "        user_query: str,\n",
    "    ):\n",
    "    # Parse the description (i.e. docstring + output fields) for each of these tools\n",
    "    docs = [\n",
    "        Document(page_content=t.description, metadata={\"index\": i})\n",
    "        for i, t in enumerate(openbb_tools)\n",
    "    ]\n",
    "\n",
    "    # Create embeddings from each of these function descriptions\n",
    "    # this will be important for when we want the agent to know what\n",
    "    # function to use for a particular query\n",
    "    vector_store = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "\n",
    "    subquestion_list = task_decomposition(user_query)\n",
    "\n",
    "    subquestions_and_tools = []\n",
    "    for subquestion in subquestion_list.subquestions:\n",
    "\n",
    "        # Tool retrieval\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={'score_threshold': 0.65}\n",
    "        )\n",
    "        docs = retriever.get_relevant_documents(subquestion.query)\n",
    "        \n",
    "        # This is a fallback mechanism in case the threshold is too high, causing too few tools to be returned.\n",
    "        # In this case, we fall back to getting the top k=2 results with higher similarity scores.\n",
    "        if len(docs) < 2:\n",
    "            retriever = vector_store.as_retriever(\n",
    "                search_kwargs={\"k\": 2}\n",
    "            )\n",
    "            \n",
    "            docs = retriever.get_relevant_documents(subquestion.query)\n",
    "            \n",
    "        tools = [openbb_tools[d.metadata[\"index\"]] for d in docs]\n",
    "\n",
    "        subquestions_and_tools.append(\n",
    "            {   \"id\": subquestion.id,\n",
    "                \"subquestion\": subquestion.question,\n",
    "                \"query\": subquestion.query,\n",
    "                \"tools\": tools,\n",
    "                \"depends_on\": subquestion.depends_on,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Go through each subquestion and create an agent with the necessary tools and context to execute on it\\n\n",
    "    for i, subquestion in enumerate(subquestions_and_tools):\n",
    "\n",
    "        # We handle each dependency manually since we don't want agents to share memory as this can go over context length\n",
    "        deps = [dep for dep in subquestions_and_tools if dep[\"id\"] in subquestion[\"depends_on\"]]\n",
    "\n",
    "        dependencies = \"\"\n",
    "        for dep in deps:\n",
    "            dependencies += \"subquestion: \" + dep[\"subquestion\"] + \"\\n\"\n",
    "            # if for some reason there's no temporal dependency between the agents being run\n",
    "            # this ensures the code doesn't break here\n",
    "            if \"observation\" in dep:\n",
    "                dependencies += \"observations:\\n\" + str(dep[\"observation\"]) + \"\\n\\n\"\n",
    "\n",
    "        input = f\"\"\"\\\n",
    "Given the following high-level question: {user_query}\n",
    "Answer the following subquestion: {subquestion['subquestion']}\n",
    "\n",
    "Give your answer in a bullet-point list.\n",
    "Explain your reasoning, and make reference to and provide the relevant retrieved data as part of your answer.\n",
    "\n",
    "Remember to use the tools provided to you to answer the question, and STICK TO THE INPUT SCHEMA.\n",
    "\n",
    "Example output format:\n",
    "```\n",
    "- <the first observation, insight, and/or conclusion> \n",
    "- <the second observation, insight, and/or conclusion> \n",
    "- <the third observation, insight, and/or conclusion> \n",
    "... REPEAT AS MANY TIMES AS NECESSARY TO ANSWER THE SUBQUESTION.\n",
    "```\n",
    "\n",
    "If necessary, make use of the following subquestions and their answers to answer your subquestion:\n",
    "{dependencies}\n",
    "\n",
    "Return only your answer as a bulleted list as a single string. Don't respond with JSON or any other kind of data structure.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            result = langchain_react_agent(tools=subquestion[\"tools\"]).invoke({\"input\": input})\n",
    "            output = result[\"output\"]\n",
    "        except Exception as err:  # Terrible practice, but it'll do for now.\n",
    "            print(err)\n",
    "            # We'll include the error message in the future\n",
    "            output = \"I was unable to answer the subquestion using the available tool.\" \n",
    "\n",
    "\n",
    "        # This is very cheeky but we are basically going into the subquestions_and_tools and for this current subquestion\n",
    "        # we are adding the output as an observation. This is important because then above we do the dependencies check-up\n",
    "        # which allows us to retrieve the correct output to be used in another subquestion.\n",
    "        # Note: this works because subquestions are done in order to execute prompt. Otherwise it wouldn't since we would\n",
    "        # be looking for an \"observation\" that doesn't exist yet.\n",
    "        subquestion[\"observation\"] = output\n",
    "\n",
    "    \n",
    "    result = verdict(\n",
    "        question=user_query,\n",
    "        subquestions=subquestions_and_tools\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "openbb_agent(openbb_tools, user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "michael",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
